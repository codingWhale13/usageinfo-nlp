# further configuration can be looked up here: https://huggingface.co/docs/transformers/v4.26.1/en/main_classes/text_generation#transformers.GenerationConfig
do_sample: True
temperature: 1.0
top_p: 1.0
num_beams: 2
repetition_penalty: 1.2
length_penalty: 1.0
num_beam_groups: 1
max_new_tokens: 512
return_dict_in_generate: True
output_scores: True

choices_per_step: 5

# do_sample: False and num_beams: 1 -> GreedySearchEncoderDecoderOutput -> scores for every token before softmax
# do_sample: False and num_beams: 2 -> BeamSearchEncoderDecoderOutput -> Beam transition scores for each vocabulary token at each generation step
# do_sample: True and num_beams: 2 -> BeamSampleEncoderDecoderOutput -> Beam transition scores for each vocabulary token at each generation step
# do_sample: True and num_beams: 1 -> SampleEncoderDecoderOutput -> scores for every token before softmax
