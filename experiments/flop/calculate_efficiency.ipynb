{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimates for GPT-4 FLOP per request:\n",
      "1.93e+12\n",
      "3.85e+12\n",
      "7.71e+12\n",
      "1.54e+13\n",
      "Total fine-tuning FLOP for T4:\n",
      "1.75e+16\n",
      "2.14e+16\n",
      "2.91e+16\n",
      "4.45e+16\n",
      "FLOP per request for T5:\n",
      "2.06e+11\n",
      "         Requests                  10 TFLOP                 100 TFLOP  \\\n",
      "0             100         17551061147016216         21405611147016216   \n",
      "1             316         17595548989582248         21450098989582248   \n",
      "2            1000         17736427157708016         21590977157708016   \n",
      "3            3162         18181717507836540         22036267507836540   \n",
      "4           10000         19590087264626016         23444637264626016   \n",
      "5           31622         24043402690379460         27897952690379460   \n",
      "6          100000         38126688333806016         41981238333806016   \n",
      "7          316227         82661284326979170         86515834326979170   \n",
      "8         1000000        223492699025606016        227347249025606016   \n",
      "9         3162277        668840100692976270        672694650692976270   \n",
      "10       10000000       2077152805943606016       2081007355943606016   \n",
      "11       31622776       6530628058390713168       6534482608390713168   \n",
      "12      100000000      20613753875123606016      20617608425123606016   \n",
      "13      316227766      65148507635368082148      65152362185368082148   \n",
      "14     1000000000     205979764566923606016     205983619116923606016   \n",
      "15     3162277660     651327302169368367336     651331156719368367336   \n",
      "16    10000000000    2059639871484923606016    2059643726034923606016   \n",
      "17    31622776601    6513115247715333453318    6513119102265333453318   \n",
      "18   100000000000   20596240940664923606016   20596244795214923606016   \n",
      "19   316227766016   65130994704204795483648   65130998558754795483648   \n",
      "20  1000000000000  205962251632464923606016  205962255487014923606016   \n",
      "\n",
      "                     1 PFLOP                  10 PFLOP  \n",
      "0          29114711147016216         44532911147016216  \n",
      "1          29159198989582248         44577398989582248  \n",
      "2          29300077157708016         44718277157708016  \n",
      "3          29745367507836540         45163567507836540  \n",
      "4          31153737264626016         46571937264626016  \n",
      "5          35607052690379460         51025252690379460  \n",
      "6          49690338333806016         65108538333806016  \n",
      "7          94224934326979170        109643134326979170  \n",
      "8         235056349025606016        250474549025606016  \n",
      "9         680403750692976270        695821950692976270  \n",
      "10       2088716455943606016       2104134655943606016  \n",
      "11       6542191708390713168       6557609908390713168  \n",
      "12      20625317525123606016      20640735725123606016  \n",
      "13      65160071285368082148      65175489485368082148  \n",
      "14     205991328216923606016     206006746416923606016  \n",
      "15     651338865819368367336     651354284019368367336  \n",
      "16    2059651435134923606016    2059666853334923606016  \n",
      "17    6513126811365333453318    6513142229565333453318  \n",
      "18   20596252504314923606016   20596267922514923606016  \n",
      "19   65131006267854795483648   65131021686054795483648  \n",
      "20  205962263196114923606016  205962278614314923606016  \n",
      "         Requests                   10 TFLOP                  100 TFLOP  \\\n",
      "0             100            192727500000000            385455000000000   \n",
      "1             316            609018900000000           1218037800000000   \n",
      "2            1000           1927275000000000           3854550000000000   \n",
      "3            3162           6094043550000000          12188087100000000   \n",
      "4           10000          19272750000000000          38545500000000000   \n",
      "5           31622          60944290050000000         121888580100000000   \n",
      "6          100000         192727500000000000         385455000000000000   \n",
      "7          316227         609456391425000000        1218912782850000000   \n",
      "8         1000000        1927275000000000000        3854550000000000000   \n",
      "9         3162277        6094577405175000000       12189154810350000000   \n",
      "10       10000000       19272750000000000000       38545500000000000000   \n",
      "11       31622776       60945785615400000000      121891571230800000000   \n",
      "12      100000000      192727500000000000000      385455000000000000000   \n",
      "13      316227766      609457867717650000000     1218915735435300000000   \n",
      "14     1000000000     1927275000000000000000     3854550000000000000000   \n",
      "15     3162277660     6094578677176500000000    12189157354353000000000   \n",
      "16    10000000000    19272750000000000000000    38545500000000000000000   \n",
      "17    31622776601    60945786773692275000000   121891573547384550000000   \n",
      "18   100000000000   192727500000000000000000   385455000000000000000000   \n",
      "19   316227766016   609457867748486400000000  1218915735496972800000000   \n",
      "20  1000000000000  1927275000000000000000000  3854550000000000000000000   \n",
      "\n",
      "                      1 PFLOP                    10 PFLOP  \n",
      "0             770910000000000            1541820000000000  \n",
      "1            2436075600000000            4872151200000000  \n",
      "2            7709100000000000           15418200000000000  \n",
      "3           24376174200000000           48752348400000000  \n",
      "4           77091000000000000          154182000000000000  \n",
      "5          243777160200000000          487554320400000000  \n",
      "6          770910000000000000         1541820000000000000  \n",
      "7         2437825565700000000         4875651131400000000  \n",
      "8         7709100000000000000        15418200000000000000  \n",
      "9        24378309620700000000        48756619241400000000  \n",
      "10       77091000000000000000       154182000000000000000  \n",
      "11      243783142461600000000       487566284923200000000  \n",
      "12      770910000000000000000      1541820000000000000000  \n",
      "13     2437831470870600000000      4875662941741200000000  \n",
      "14     7709100000000000000000     15418200000000000000000  \n",
      "15    24378314708706000000000     48756629417412000000000  \n",
      "16    77091000000000000000000    154182000000000000000000  \n",
      "17   243783147094769100000000    487566294189538200000000  \n",
      "18   770910000000000000000000   1541820000000000000000000  \n",
      "19  2437831470993945600000000   4875662941987891200000000  \n",
      "20  7709100000000000000000000  15418200000000000000000000  \n",
      "         Requests                    10 TFLOP                   100 TFLOP  \\\n",
      "0             100           17358333647016216           21020156147016216   \n",
      "1             316           16986530089582248           20232061189582248   \n",
      "2            1000           15809152157708016           17736427157708016   \n",
      "3            3162           12087673957836540            9848180407836540   \n",
      "4           10000             317337264626016          -15100862735373984   \n",
      "5           31622          -36900887359620540          -93990627409620540   \n",
      "6          100000         -154600811666193984         -343473761666193984   \n",
      "7          316227         -526795107098020830        -1132396948523020830   \n",
      "8         1000000        -1703782300974393984        -3627202750974393984   \n",
      "9         3162277        -5425737304482023730       -11516460159657023730   \n",
      "10       10000000       -17195597194056393984       -36464492644056393984   \n",
      "11       31622776       -54415157557009286832      -115357088622409286832   \n",
      "12      100000000      -172113746124876393984      -364837391574876393984   \n",
      "13      316227766      -544309360082281917852     -1153763373249931917852   \n",
      "14     1000000000     -1721295235433076393984     -3648566380883076393984   \n",
      "15     3162277660     -5443251375007131632664    -11537826197633631632664   \n",
      "16    10000000000    -17213110128515076393984    -36485856273965076393984   \n",
      "17    31622776601    -54432671525976941546682   -115378454445119216546682   \n",
      "18   100000000000   -172131259059335076393984   -364858755204785076393984   \n",
      "19   316227766016   -544326873044281604516352  -1153784736938218004516352   \n",
      "20  1000000000000  -1721312748367535076393984  -3648587744512985076393984   \n",
      "\n",
      "                       1 PFLOP                     10 PFLOP  \n",
      "0            28343801147016216            42991091147016216  \n",
      "1            26723123389582248            39705247789582248  \n",
      "2            21590977157708016            29300077157708016  \n",
      "3             5369193307836540            -3588780892163460  \n",
      "4           -45937262735373984          -107610062735373984  \n",
      "5          -208170107509620540          -436529067709620540  \n",
      "6          -721219661666193984         -1476711461666193984  \n",
      "7         -2343600631373020830         -4766007997073020830  \n",
      "8         -7474043650974393984        -15167725450974393984  \n",
      "9        -23697905870007023730        -48060797290707023730  \n",
      "10       -75002283544056393984       -152077865344056393984  \n",
      "11      -237240950753209286832       -481008675014809286832  \n",
      "12      -750284682474876393984      -1521179264274876393984  \n",
      "13     -2372671399585231917852      -4810487452255831917852  \n",
      "14     -7503108671783076393984     -15212193253583076393984  \n",
      "15    -23726975842886631632664     -48105275133392631632664  \n",
      "16    -75031348564865076393984    -152122333146665076393984  \n",
      "17   -237270020283403766546682    -481053151959972866546682  \n",
      "18   -750313747495685076393984   -1521223732077485076393984  \n",
      "19  -2372700464726090804516352   -4810531920301836404516352  \n",
      "20  -7503137736803885076393984  -15212237721385685076393984  \n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "TRAINING_FLOP = 13675914923606016\n",
    "LABELED_DATAPOINTS = 2000\n",
    "FLOP_PER_REQUEST_T5 = round(205962234101.76)\n",
    "TOKENS_PER_REQUEST_T5 = 23840 / 4000\n",
    "FLOP_PER_TOKEN_GPT3 = 350e9\n",
    "TOKENS_PER_REQUEST_GPT4 = 5.5065\n",
    "\n",
    "# Starting at 10 TFLOP per forward pass\n",
    "gpt4_flop_per_request_estimates = [\n",
    "    (int)(TOKENS_PER_REQUEST_GPT4 * FLOP_PER_TOKEN_GPT3 * (2 ** x)) for x in range(4)\n",
    "]\n",
    "# gpt4_flop_per_request_estimates = [(int)(TOKENS_PER_REQUEST_GPT4*560e12), 0, 0, 0]\n",
    "\n",
    "print(\"Estimates for GPT-4 FLOP per request:\")\n",
    "print(*[f\"{x:.2e}\" for x in gpt4_flop_per_request_estimates], sep='\\n')\n",
    "print(\"Total fine-tuning FLOP for T4:\")\n",
    "print(*[f\"{TRAINING_FLOP + LABELED_DATAPOINTS * x:.2e}\" for x in gpt4_flop_per_request_estimates], sep='\\n')\n",
    "print(\"FLOP per request for T5:\")\n",
    "print(f\"{FLOP_PER_REQUEST_T5:.2e}\")\n",
    "\n",
    "num_requests = [(int)(10 ** (x / 2)) for x in range(4, 25)]\n",
    "\n",
    "\n",
    "def total_flop_t5(requests: int, flop_per_request_gpt4) -> int:\n",
    "    fine_tuning_flop = TRAINING_FLOP + LABELED_DATAPOINTS * flop_per_request_gpt4\n",
    "    # print fine_tuning_flop in scientific notation\n",
    "    inference_flop = requests * FLOP_PER_REQUEST_T5\n",
    "    return fine_tuning_flop + inference_flop\n",
    "\n",
    "\n",
    "def total_flop_gpt4(requests: int, flop_per_request_gpt4) -> int:\n",
    "    return requests * flop_per_request_gpt4\n",
    "\n",
    "\n",
    "labels = [\"10 TFLOP\", \"100 TFLOP\", \"1 PFLOP\", \"10 PFLOP\"]\n",
    "df_t5 = pd.DataFrame(num_requests, columns=[\"Requests\"])\n",
    "df_gpt4 = pd.DataFrame(num_requests, columns=[\"Requests\"])\n",
    "\n",
    "for label, flop_per_request_gpt4 in zip(labels, gpt4_flop_per_request_estimates):\n",
    "    df_t5[label] = df_t5[\"Requests\"].apply(\n",
    "        lambda x: total_flop_t5(x, flop_per_request_gpt4)\n",
    "    )\n",
    "    df_gpt4[label] = df_gpt4[\"Requests\"].apply(\n",
    "        lambda x: total_flop_gpt4(x, flop_per_request_gpt4)\n",
    "    )\n",
    "\n",
    "df_diff = df_t5 - df_gpt4\n",
    "df_diff[\"Requests\"] = df_t5[\"Requests\"]\n",
    "\n",
    "print(df_t5)\n",
    "print(df_gpt4)\n",
    "print(df_diff)\n",
    "\n",
    "#plot for 100 TFLOP, df_t5 and df_gpt4\n",
    "# df_t5.plot(x=\"Requests\", y=\"100 TFLOP\", title=\"T5\", logx=True, logy=True)\n",
    "# df_gpt4.plot(x=\"Requests\", y=\"100 TFLOP\", title=\"GPT-4\", logx=True, logy=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Savings for 10 TFLOP at 10k: 0.317337264626016\n",
      "Break-even point for 10 TFLOP: 10185\n",
      "Savings for 100 TFLOP at 10k: -15.100862735373983\n",
      "Break-even point for 100 TFLOP: 5862\n",
      "Savings for 1 PFLOP at 10k: -45.937262735373984\n",
      "Break-even point for 1 PFLOP: 3878\n",
      "Savings for 10 PFLOP at 10k: -107.61006273537399\n",
      "Break-even point for 10 PFLOP: 2927\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#compute break-even point for 10 TFLOP, 100 TFLOP, 1 PFLOP, 10 PFLOP\n",
    "\n",
    "\n",
    "def cost_t5(gpt_4_estimate: int, num_requests) -> int:\n",
    "    estimate_t5 = TRAINING_FLOP + (LABELED_DATAPOINTS * gpt_4_estimate) + (num_requests * FLOP_PER_REQUEST_T5)\n",
    "    return estimate_t5\n",
    "\n",
    "def cost_gpt4(gpt_4_estimate: int, num_requests) -> int:\n",
    "    estimate_gpt4 = num_requests * gpt_4_estimate \n",
    "    return estimate_gpt4\n",
    "\n",
    "\n",
    "def diff_t5_gpt_4(gpt_4_estimate: int, num_requests: int):\n",
    "    return cost_t5(gpt_4_estimate, num_requests) - cost_gpt4(gpt_4_estimate, num_requests)\n",
    "\n",
    "break_even_points = []\n",
    "#loop until break-even point for all estimates\n",
    "for label, flop_per_request_gpt4 in zip(labels, gpt4_flop_per_request_estimates):\n",
    "    num_requests = 1\n",
    "    while True:\n",
    "        diff = diff_t5_gpt_4(flop_per_request_gpt4, num_requests)\n",
    "        if diff < 0:\n",
    "            break_even_points.append(num_requests)\n",
    "            break\n",
    "        num_requests += 1\n",
    "\n",
    "#compute training cost for all estimates in PFLOP\n",
    "for label, flop_per_request_gpt4 in zip(labels, gpt4_flop_per_request_estimates):\n",
    "    #print(f\"Training only cost for {label}: {TRAINING_FLOP / 10**15}\")\n",
    "    #print(f\"Training only cost for {label} in PFLOP: {LABELED_DATAPOINTS * flop_per_request_gpt4 / 10**15}\")\n",
    "    #print(f\"Training cost for {label}: {(TRAINING_FLOP + (LABELED_DATAPOINTS * flop_per_request_gpt4)) / 10**15}\")\n",
    "    #print(f\"Inference cost T5 for {label}: {FLOP_PER_REQUEST_T5 / 10**15}\")\n",
    "    #print(f\"Inference cost GPT 4 for {label}: {flop_per_request_gpt4 / 10**15}\")\n",
    "    print(f\"Savings for {label} at 10k: {diff_t5_gpt_4(flop_per_request_gpt4, 10000) / 10**15}\")\n",
    "    print(f\"Break-even point for {label}: {break_even_points.pop(0)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request FLOP: 0.000205962234102 PFLOP\n",
      "Training FLOP: 13.675914923606015 PFLOP\n",
      "Training Data FLOP: 11.92 PFLOP\n",
      "Request FLOP GPT-4: 0.00596 PFLOP\n",
      "         Requests            T5         GPT-4\n",
      "0             100  2.561651e+16  5.960000e+14\n",
      "1             316  2.566100e+16  1.883360e+15\n",
      "2            1000  2.580188e+16  5.960000e+15\n",
      "3            3162  2.624717e+16  1.884552e+16\n",
      "4           10000  2.765554e+16  5.960000e+16\n",
      "5           31622  3.210885e+16  1.884671e+17\n",
      "6          100000  4.619214e+16  5.960000e+17\n",
      "7          316227  9.072673e+16  1.884713e+18\n",
      "8         1000000  2.315581e+17  5.960000e+18\n",
      "9         3162277  6.769056e+17  1.884717e+19\n",
      "10       10000000  2.085218e+18  5.960000e+19\n",
      "11       31622776  6.538694e+18  1.884717e+20\n",
      "12      100000000  2.062182e+19  5.960000e+20\n",
      "13      316227766  6.515657e+19  1.884717e+21\n",
      "14     1000000000  2.059878e+20  5.960000e+21\n",
      "15     3162277660  6.513354e+20  1.884717e+22\n",
      "16    10000000000  2.059648e+21  5.960000e+22\n",
      "17    31622776601  6.513123e+21  1.884717e+23\n",
      "18   100000000000  2.059625e+22  5.960000e+23\n",
      "19   316227766016  6.513100e+22  1.884717e+24\n",
      "20  1000000000000  2.059623e+23  5.960000e+24\n"
     ]
    }
   ],
   "source": [
    "TRAINING_FLOP = 13675914923606016\n",
    "FLOP_PER_REQUEST_T5 = round(205962234101.76)\n",
    "TOKENS_PER_REQUEST_T5 = 23840 / 4000\n",
    "\n",
    "# If we estimate that GPT-4 with 100 TFLOP per token\n",
    "GPT4_per_request = 10**12 * TOKENS_PER_REQUEST_T5\n",
    "\n",
    "#compute to TFLOP\n",
    "\n",
    "#compute to PFLOP\n",
    "pflop = TRAINING_FLOP / 10**15\n",
    "pflop_per_request_t5 = FLOP_PER_REQUEST_T5 / 10**15\n",
    "pflop_training_data = LABELED_DATAPOINTS * GPT4_per_request / 10**15\n",
    "pflop_per_request_gpt4 = GPT4_per_request / 10**15\n",
    "print(f\"Request FLOP: {pflop_per_request_t5} PFLOP\")\n",
    "print(f\"Training FLOP: {pflop} PFLOP\")\n",
    "print(f\"Training Data FLOP: {pflop_training_data} PFLOP\")\n",
    "print(f\"Request FLOP GPT-4: {pflop_per_request_gpt4} PFLOP\")\n",
    "\n",
    "def total_flop_t5(requests: int) -> int:\n",
    "    fine_tuning_flop = TRAINING_FLOP + LABELED_DATAPOINTS * GPT4_per_request\n",
    "    inference_flop = requests * FLOP_PER_REQUEST_T5\n",
    "    return fine_tuning_flop + inference_flop\n",
    "\n",
    "def total_flop_gpt4(requests: int) -> int:\n",
    "    return requests * GPT4_per_request\n",
    "\n",
    "num_requests = [(int)(10 ** (x / 2)) for x in range(4, 25)]\n",
    "\n",
    "cost_t5 = [total_flop_t5(x) for x in num_requests]\n",
    "cost_gpt4 = [total_flop_gpt4(x) for x in num_requests]\n",
    "\n",
    "df = pd.DataFrame(num_requests, columns=[\"Requests\"])\n",
    "df[\"T5\"] = cost_t5\n",
    "df[\"GPT-4\"] = cost_gpt4\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
