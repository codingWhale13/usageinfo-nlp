accumulate_grad_batches: 16
active_layers:
  decoder: "-3:" # use python slice syntax (e.g. ':0' for no layers and ':' for all)
  encoder: "-8:"
  lm_head: True
active_learning:
 module: "ActiveDataModule"
 parameters:
artifact:
  checkpoint: #9
  name: #"scintillating-mandu-99"
batch_size: 8
cluster:
  devices: 1 #Number of gpus per node
  num_nodes: 1 #Number of nodes
dataset:
  test_set:
<<<<<<< HEAD
    name: "ba-30k-test"
=======
    name: "silver-test-69"
>>>>>>> f382eb52 (experiments [feat]: add BP config)
  training_set:
    augmentation_set: # "legacy"
    dataloader_setup_seed: 42
    drop_out: 0.0 # fraction of data to drop from training set (validation set stays untouched)
    name: "ba-30k-train"
    outlier_removal:
      percentage: 0.0 # fraction of data to remove from training set (validation set stays untouched), use only when drop_out is 0.0
      distance_threshold: 0.53 # distance threshold for outlier removal, default is 0.53
    stratified_drop_out: True # stratification based on if the review has usage options or not
    usage_split: # 0.5
    validation_split: 0.00
  validation_set:
    name: "ba-30k-val" # if you choose a specific validation set, the validation split will be ignored
epochs: 15
gradual_unfreezing_mode: "" # "module speed" for each module (seperated by comma) with module=[decoder, encoder], speed int NOT 0
lr_scheduler_type: OneCycleLR #Currently supports OneCycleLR and AdaFactor. If you do not want a LR scheduler write null
model_name: "flan-t5-base"
multiple_usage_options_strategy: "default"
optimizer: #Set all parameters you do not want to null
  lr: 0.0001
  name: "AdamW"
  relative_step: False
  scale_parameter: False
  warmup_init: null
  weight_decay: 0.01
prompt_id: "original"
seed: 42 # can be left empty
<<<<<<< HEAD
test_run: False
=======
test_run: True
>>>>>>> f382eb52 (experiments [feat]: add BP config)

# Options for 'multiple_usage_options_strategy':
#   - "default" -> wie gehabt kommt ein Datapoint raus, der alle usage options in gegebener Reihenfolge enthält
#   - "flat" -> ein Datapoint für jede usage option
#   - "shuffle" -> wie bei "default" ein Datapoint, aber mit zufälliger Reihenfolge
#   - "shuffle-n" -> n zufällige Datapoints mit jeweils unterschiedlichen zufälligen Permutationen
#   - "shuffle-all" -> alle Permutationen als separate Datapoints

# Further examples for 'active_layers' syntax:
#   - ':0' -> activate none
#   - ':' -> activate all ('0:') also works
#   - '-5:' -> activate last five
#   - ':3' -> activate first three
